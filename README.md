# Data Warehouse with AWS Redshift

## Introduction
A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

## Objectives
The project objective is to create a data model using Python and AWS Redshift to simulate a real project at Sparkify. Sparkify team wants to analyze the data they've been collecting on songs and user activity on their new music streaming application, but they don't have a data warehouse and all data are available only in JSON files generated by the app.

## Modeling
Running ```create_tables.py``` we are going to create two staging tables we need to organize and struct data, reading JSON files from S3:

    1. public.staging_events: Sparkify events in the app
    2. public.staging_songs: Sparkify songs metadata

Using staging tables, we are able to create a star schema in Redshift with the following tables:

    1. public.users (DIMENSION): user_id, first_name, last_name, gender, level
    2. public.artists (DIMENSION): artist_id, name, location, latitude, longitude
    3. public.songs (DIMENSION): song_id, title, artist_id, year, duration
    4. public.time (DIMENSION): start_time, hour, day, week, month, year, weekday
    5. public.songplays (FACT): songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

With them, the next step is to run ```etl.py``` to read JSON files, process data and write into the tables above.

## Star Schema
![Entity Relationship Diagram (3)](https://github.com/leonardodrigo/udacity-cloud-datawarehouse/blob/main/songplays.jpg))


